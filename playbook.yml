# run this playbook like so: `ansible-playbook playbook.yml --ask-pass --ask-become-pass`. The ask-pass bit is necessary until you've got your ssh key installed
---
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: test connection
      ping:
    - name: apt update
      apt: update_cache=yes
      become: true
    - name: install vim
      apt: name=vim
      become: true
      
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: empty hostname file
      command: "truncate /etc/hostname --size=0"
      become: true
    
    - name: update hostname file
      replace:
        dest: /etc/hostname
        regexp: "^.*$"
        replace: "ubuntu-standard-{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      become: true
      
    - name: set hostname for session
      command: "hostname ubuntu-standard-{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      become: true
      
    - name: ensure all nodes are in each other's hosts files
      lineinfile:
        dest: /etc/hosts
        state: present
        line: "{{ hostvars[item]['ansible_default_ipv4']['address'] }} ubuntu-standard-{{ hostvars[item]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      with_items: play_hosts
      become: true
      
      #see https://github.com/ansible/ansible-modules-core/blob/devel/system/authorized_key.py
    - name: install ssh key
      authorized_key:
            user: ubuntu
            key: "{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}"
            state: present
            exclusive: true

- hosts: cluster-pis
  remote_user: ubuntu
  vars:
        # java_version discovered by running `apt-cache show openjdk-8-jre-headless` on the remote machine
        java_major_version: 8
        java_version: 8u91-b14-3ubuntu1~16.04.1
        ansible_architecture: armhf
        spark_user: spark
        spark_user_groups: [ubuntu]
  
  # try to install spark
  roles:
    - 
      role: azavea.spark
      become: true

# the master device should have ssh key access to the workers. 
# (https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)
# (https://docs.ansible.com/ansible/playbooks_conditionals.html#register-variables)
- hosts: cluster-pi-master
  remote_user: ubuntu
  tasks:
    - name: check for an ssh key
      stat: path="~/.ssh/id_rsa.pub"
      register: master_ssh_key_stat

    - name: ensure an ssh id key is created on the master(s)
      command: ssh-keygen -f ~/.ssh/id_rsa -N ""
      when: master_ssh_key_stat.stat.exists == false
      
    - name: get the master's ssh public key
      command: cat ~/.ssh/id_rsa.pub
      register: master_ssh_pubkey

- hosts: cluster-pi-workers
  remote_user: ubuntu
  tasks:
    # now install the master's public id as ssh authorised on the workers
    # https://stackoverflow.com/questions/33896847/how-do-i-set-register-a-variable-to-persist-between-plays-in-ansible/33903682#33903682
    # https://docs.ansible.com/ansible/playbooks_variables.html#magic-variables-and-how-to-access-information-about-other-hosts
    - name: install master ssh public key on workers
      authorized_key:
        user: ubuntu
        key: "{{ hostvars[item]['master_ssh_pubkey'].stdout }}"
        state: present
        exclusive: false
        # exclusive is false because for now all the nodes have the same username
      with_items: "{{ groups['cluster-pi-master'] }}"
      
- hosts: cluster-pi-master
  remote_user: ubuntu
  vars:
    spark_conf_dir: /usr/lib/spark/conf/
  tasks:
    - name: create the workers file on the master
      command: "cp -f {{ spark_conf_dir }}slaves.template {{ spark_conf_dir }}slaves"
      
    - name: add the workers to the master's list
      lineinfile:
        dest: "{{ spark_conf_dir }}slaves"
        state: present
        line: "{{ item }}"
      with_items: "{{ groups['cluster-pi-workers'] }}"
...
