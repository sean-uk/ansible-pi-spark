# run this playbook like so: `ansible-playbook playbook.yml --ask-pass --ask-become-pass`. The ask-pass bit is necessary until you've got your ssh key installed
---
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: apt update
      apt: update_cache=yes
      become: true
    - name: install vim
      apt: name=vim
      become: true
      
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: choose a unique hostname for each of the nodes
      set_fact:
        cluster_hostname: ubuntu-standard-{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}
      
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
      #see https://github.com/ansible/ansible-modules-core/blob/devel/system/authorized_key.py
    - name: install the provisioner's ssh key into the nodes
      authorized_key:
            user: ubuntu
            key: "{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}"
            state: present
            exclusive: true
  
      # dunno why, but the first time you restart, eth0 seems to vanish and be replaced by this mac-address generated version
    - name: change eth0 to 'biosdevname' device (I)
      replace:
        dest: /etc/network/interfaces
        regexp: "^auto eth0$"
        replace: "auto enx{{ hostvars[inventory_hostname]['ansible_default_ipv4']['macaddress']|regex_replace(':', '') }}"
      become: true
      
    - name: change eth0 to 'biosdevname' device (II)
      replace:
        dest: /etc/network/interfaces
        regexp: "^iface eth0 inet dhcp$"
        replace: "iface enx{{ hostvars[inventory_hostname]['ansible_default_ipv4']['macaddress']|regex_replace(':', '') }} inet dhcp"
      become: true
  
    - name: empty hostname file
      command: "truncate /etc/hostname --size=0"
      become: true
    
    - name: update hostname file
      replace:
        dest: /etc/hostname
        regexp: "^.*$"
        replace: "{{ cluster_hostname }}"
      become: true
      
    - name: set hostname for session
      command: "hostname {{ cluster_hostname }}"
      become: true
      
    - name: ensure all nodes are in each other's hosts files
      lineinfile:
        dest: /etc/hosts
        state: present
        line: "{{ hostvars[item]['ansible_default_ipv4']['address'] }} {{ hostvars[item]['cluster_hostname'] }}"
      with_items: play_hosts
      become: true
      
    - name: check for an ssh key
      stat: path="~/.ssh/id_rsa.pub"
      register: ssh_id_check

    - name: ensure an ssh id key exists
      command: ssh-keygen -f ~/.ssh/id_rsa -N ""
      when: ssh_id_check.stat.exists == false
      
    - name: learn the node's ssh id
      command: cat ~/.ssh/id_rsa.pub
      register: ssh_id
      
      # spark's ./sbin/stop-all.sh depends on ssh access.
    - name: allow nodes to ssh into themselves by key
      authorized_key:
        user: ubuntu
        key: "{{ ssh_id.stdout }}"
        state: present
        exclusive: false

- hosts: cluster-pi-bulk-storage
  remote_user: ubuntu
  vars:
    cluster_pi_bulk_storage_dir: /mnt/cluster_pi_bulk_storage
  tasks:
    - name: ensure bulk storage dir exists
      file:
        path: "{{ cluster_pi_bulk_storage_dir }}"
        state: directory
        owner: ubuntu
        group: spark
        mode: 0770
      become: true
  
      # https://superuser.com/questions/361885/how-do-i-figure-out-which-dev-is-a-usb-flash-drive/361892#361892
    - name: check for usb mass storage
      shell: find /dev/disk/by-id/ | grep usb.*-part
      register: usb_devices
      
    - name: get the first mass storage device name
      set_fact:
        cluster_bulk_storage: "{{ cluster_pi_bulk_storage_dir }}"
      
      # todo: learn the FS type.
    - name: mount the usb mass storage on startup
      mount:
        name: "{{ cluster_pi_bulk_storage_dir }}"
        src: "{{ usb_devices.stdout_lines | first }}"
        state: mounted
        fstype: ext4
      become: true

- hosts: cluster-pi-bulk-storage
  remote_user: ubuntu
  roles:
    - role: geerlingguy.nfs
      become: true
  vars:
    nfs_exports:
      - "{{ hostvars[inventory_hostname]['cluster_bulk_storage'] }} *(rw,sync,no_root_squash)"
      
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: create a folder for the host on the bulk storage
      file:
        path: "{{item}}/{{ cluster_hostname }}"
        state: directory
        owner: ubuntu
        group: spark
        mode: 0770
      become: true
      with_items: groups['cluster-pi-bulk-storage']
      
    #~ - name: tell the hosts to mount bulk storage on startup
      #~ mount:
        #~ name: "{{item}}/{{ cluster_hostname }}"
        #~ src: "{{ usb_devices.stdout_lines | first }}"
        #~ state: mounted
        #~ fstype: ext4
      #~ become: true
      #~ with_items: groups['cluster-pi-bulk-storage']

- hosts: cluster-pi-workers
  remote_user: ubuntu
  tasks:
    # now install the master's public id as ssh authorised on the workers
    # https://stackoverflow.com/questions/33896847/how-do-i-set-register-a-variable-to-persist-between-plays-in-ansible/33903682#33903682
    # https://docs.ansible.com/ansible/playbooks_variables.html#magic-variables-and-how-to-access-information-about-other-hosts
    - name: install master ssh public key on workers
      authorized_key:
        user: ubuntu
        key: "{{ hostvars[item]['ssh_id'].stdout }}"
        state: present
        exclusive: false
        # exclusive is false because for now all the nodes have the same username
      with_items: groups['cluster-pi-master']

- hosts: cluster-pis
  remote_user: ubuntu
  
  # try to install spark
  roles:
    - 
      role: azavea.spark
      become: true
  
  vars:
        # java_version discovered by running `apt-cache show openjdk-8-jre-headless` on the remote machine
        java_major_version: 8
        java_version: 8u91-b14-3ubuntu1~16.04.1
        ansible_architecture: armhf
        spark_user: spark
        spark_user_groups: [ubuntu]
        
  tasks:
    - name: create the workers file on the master
      command: "cp -f {{ spark_conf_dir }}/conf/slaves.template {{ spark_conf_dir }}/conf/slaves"
      when: inventory_hostname in groups['cluster-pi-master']
      
    - name: add the workers to the master's list
      lineinfile:
        dest: "{{ spark_conf_dir }}/conf/slaves"
        state: present
        line: "{{ item }}"
      with_items: "{{ groups['cluster-pi-workers'] }}"
      when: inventory_hostname in groups['cluster-pi-master']
      
      # (want to add a worker on the master node with different parameters)
    - name: take localhost out of the workers list
      lineinfile:
        dest: "{{ spark_conf_dir }}/conf/slaves"
        state: absent
        line: "localhost"
      when: inventory_hostname in groups['cluster-pi-master']
      
    - name: stop any currently running spark processes
      command: "{{ spark_usr_dir }}/sbin/stop-all.sh"
      
    - name: start master node
      command: "{{ spark_usr_dir }}/sbin/start-master.sh"
      when: inventory_hostname in groups['cluster-pi-master']

      # starting worker with less resources on the master node.
      # todo: actually find the spark URL(s) rather than guessing from the hostname
    - name: start worker on master node
      command: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://{{ hostvars[item]['cluster_hostname'] }}:7077 --cores 2"
      with_items: groups['cluster-pi-master']
      when: inventory_hostname in groups['cluster-pi-master']
      
      ## todo: don't think this'll work right if there ever were more than one master!
    - name: start workers on worker nodes
      command: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://u{{ hostvars[item]['cluster_hostname'] }}:7077"
      with_items: groups['cluster-pi-master']
      when: inventory_hostname in groups['cluster-pi-workers']
        
    - name: tell the master node's master job to start at reboot
      cron:
        user: ubuntu
        name: start-spark-master
        job: "{{ spark_usr_dir }}/sbin/start-master.sh"
        special_time: reboot
      when: inventory_hostname in groups['cluster-pi-master']
        
    - name: tell master node's worker job to start at reboot
      cron:
        user: ubuntu
        name: start-spark-worker
        job: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://{{ hostvars[inventory_hostname]['cluster_hostname'] }}:7077 --cores 2"
        special_time: reboot
      when: inventory_hostname in groups['cluster-pi-master']
      
    - name: tell worker nodes worker jobs to start at reboot
      cron:
        name: start-spark-worker
        user: ubuntu
        job: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://{{ hostvars[item]['cluster_hostname'] }}:7077"
        special_time: reboot
      with_items: groups['cluster-pi-master']
      when: inventory_hostname in groups['cluster-pi-workers']
...
