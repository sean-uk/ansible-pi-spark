# run this playbook like so: `ansible-playbook playbook.yml --ask-pass --ask-become-pass`. The ask-pass bit is necessary until you've got your ssh key installed
---
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
    - name: test connection
      ping:
    - name: apt update
      apt: update_cache=yes
      become: true
    - name: install vim
      apt: name=vim
      become: true
      
- hosts: cluster-pis
  remote_user: ubuntu
  tasks:
      #see https://github.com/ansible/ansible-modules-core/blob/devel/system/authorized_key.py
    - name: install ssh key
      authorized_key:
            user: ubuntu
            key: "{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}"
            state: present
            exclusive: true
  
      # dunno why, but the first time you restart, eth0 seems to vanish and be replaced by this mac-address generated version
    - name: change eth0 to 'biosdevname' device (I)
      replace:
        dest: /etc/network/interfaces
        regexp: "^auto eth0$"
        replace: "auto enx{{ hostvars[inventory_hostname]['ansible_default_ipv4']['macaddress']|regex_replace(':', '') }}"
      become: true
      
    - name: change eth0 to 'biosdevname' device (II)
      replace:
        dest: /etc/network/interfaces
        regexp: "^iface eth0 inet dhcp$"
        replace: "iface enx{{ hostvars[inventory_hostname]['ansible_default_ipv4']['macaddress']|regex_replace(':', '') }} inet dhcp"
      become: true
  
    - name: empty hostname file
      command: "truncate /etc/hostname --size=0"
      become: true
    
    - name: update hostname file
      replace:
        dest: /etc/hostname
        regexp: "^.*$"
        replace: "ubuntu-standard-{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      become: true
      
    - name: set hostname for session
      command: "hostname ubuntu-standard-{{ hostvars[inventory_hostname]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      become: true
      
    - name: ensure all nodes are in each other's hosts files
      lineinfile:
        dest: /etc/hosts
        state: present
        line: "{{ hostvars[item]['ansible_default_ipv4']['address'] }} ubuntu-standard-{{ hostvars[item]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}"
      with_items: play_hosts
      become: true
      
# the master device should have ssh key access to the workers. 
# (https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)
# (https://docs.ansible.com/ansible/playbooks_conditionals.html#register-variables)
- hosts: cluster-pi-master
  remote_user: ubuntu
  tasks:
    - name: check for an ssh key
      stat: path="~/.ssh/id_rsa.pub"
      register: master_ssh_key_stat

    - name: ensure an ssh id key is created on the master(s)
      command: ssh-keygen -f ~/.ssh/id_rsa -N ""
      when: master_ssh_key_stat.stat.exists == false
      
    - name: get the master's ssh public key
      command: cat ~/.ssh/id_rsa.pub
      register: master_ssh_pubkey

- hosts: cluster-pi-workers
  remote_user: ubuntu
  tasks:
    # now install the master's public id as ssh authorised on the workers
    # https://stackoverflow.com/questions/33896847/how-do-i-set-register-a-variable-to-persist-between-plays-in-ansible/33903682#33903682
    # https://docs.ansible.com/ansible/playbooks_variables.html#magic-variables-and-how-to-access-information-about-other-hosts
    - name: install master ssh public key on workers
      authorized_key:
        user: ubuntu
        key: "{{ hostvars[item]['master_ssh_pubkey'].stdout }}"
        state: present
        exclusive: false
        # exclusive is false because for now all the nodes have the same username
      with_items: "{{ groups['cluster-pi-master'] }}"

- hosts: cluster-pis
  remote_user: ubuntu
  
  #~ # try to install spark
  roles:
    - 
      role: azavea.spark
      become: true
  
  vars:
        # java_version discovered by running `apt-cache show openjdk-8-jre-headless` on the remote machine
        java_major_version: 8
        java_version: 8u91-b14-3ubuntu1~16.04.1
        ansible_architecture: armhf
        spark_user: spark
        spark_user_groups: [ubuntu]
        
  tasks:
    - name: make spark log dir writable by standard user.
      file: path="{{ spark_log_dir }}" state="directory" owner=ubuntu
      become: true
    - name: make spark run dir writable by standard user.
      file: path="{{ spark_run_dir }}" state="directory" owner=ubuntu
      become: true
      
    - name: create the workers file on the master
      command: "cp -f {{ spark_conf_dir }}/conf/slaves.template {{ spark_conf_dir }}/conf/slaves"
      when: inventory_hostname in groups['cluster-pi-master']
      
    - name: add the workers to the master's list
      lineinfile:
        dest: "{{ spark_conf_dir }}/conf/slaves"
        state: present
        line: "{{ item }}"
      with_items: "{{ groups['cluster-pi-workers'] }}"
      when: inventory_hostname in groups['cluster-pi-master']
      
      # (want to add a worker on the master node with different parameters)
    - name: take localhost out of the workers list
      lineinfile:
        dest: "{{ spark_conf_dir }}/conf/slaves"
        state: absent
        line: "localhost"
      when: inventory_hostname in groups['cluster-pi-master']
      
    - name: start master node
      command: "{{ spark_usr_dir }}/sbin/start-master.sh"
      when: inventory_hostname in groups['cluster-pi-master']

      # starting worker with less resources on the master node.
      # todo: actually find the spark URL(s) rather than guessing from the hostname
    - name: start worker on master node
      command: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://ubuntu-standard-{{ hostvars[item]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}:7077 --cores 2"
      with_items: groups['cluster-pi-master']
      when: inventory_hostname in groups['cluster-pi-master']
      
      ## don't think this'll work right if there ever were more than one master!
    - name: start workers on worker nodes
      command: "{{ spark_usr_dir }}/sbin/start-slave.sh 1 spark://ubuntu-standard-{{ hostvars[item]['ansible_default_ipv4']['address']|regex_replace('[^a-zA-Z0-9]', '-') }}:7077"
      with_items: groups['cluster-pi-master']
      when: inventory_hostname in groups['cluster-pi-workers']
...
